### YamlMime:FAQ
metadata:
  title: Azure AI model inference service frequently asked questions
  titleSuffix: Azure AI studio
  description: Get answers to the most popular questions about Azure AI model service
  #services: cognitive-services
  manager: nitinme
  ms.service: azure-ai-studio
  ms.topic: faq
  ms.date: 08/13/2024
  ms.author: fasantia
  author: santiagxf
title: Azure AI model inference service frequently asked questions
summary: |
  If you can't find answers to your questions in this document, and still need help check the [Azure AI services support options guide](../cognitive-services-support-options.md?context=/azure/ai-services/openai/context/context).
sections:
  - name: General
    questions:
      - question: |
          What's the difference between Azure OpenAI service and Azure AI model inference service?
        answer: |
          Azure OpenAI Service give customers access to advanced language models from OpenAI. Azure AI model inference service extends such capability giving customers access to all the flagship models in Azure AI, including Azure OpenAI, Cohere, Mistral AI, Meta Llama, AI21 labs, etc; under the same service, endpoint, and credentials. Customers can seamlessly switch between models without changing their code.

          Both Azure OpenAI Service and Azure AI model inference service are part of the Azure AI services family and build on top of the same security and enterprise promise of Azure.

          While Azure AI model inference service focus on inference, Azure OpenAI Service can be used with more advanced APIs like batch, fine-tuning, assistants, and files.
      - question: |
          What's the difference between OpenAI and Azure OpenAI?
        answer: |
          Azure AI Models and Azure OpenAI Service give customers access to advanced language models from OpenAI with the security and enterprise promise of Azure. Azure OpenAI co-develops the APIs with OpenAI, ensuring compatibility and a smooth transition from one to the other.

          Customers get the security capabilities of Microsoft Azure while running the same models as OpenAI. It offers private networking, regional availability, and responsible AI content filtering.  

          Learn more about the [Azure OpenAI service](../../openai/overview.md).
      - question: |
          What's the difference between Azure AI services and Azure AI studio?
        answer: |
          Azure AI services is a suite of AI services that provide pre-built APIs for common AI scenarios. Azure AI studio is a web-based tool that allows you to build, train, and deploy machine learning models. Azure AI services can be used in Azure AI studio to enhance your models with pre-built AI capabilities.
  - name: Models
    questions:
      - question: |
          Why are not all the models in the Azure AI model catalog supported in Azure AI services?
        answer: |
          The Azure AI model inference service in AI services support all the models in the Azure AI catalog having pay-as-you-go billing. See [the Models article](concepts/models.md) for more information.

          The Azure AI model catalog contains a wider list of models, however, those models require compute quota from your subscription. They also need to have a project or AI hub where to host the deployment. See [deployment options in Azure AI studio](../concepts/deployments-overview.md) for more details.
      - question: |
          Why I can't add OpenAI o1-preview or OpenA o1-mini-preview to my resource?
        answer: |
          The Azure OpenAI Service o1 models require registration and are eligible only to customers on the Enterprise Agreement Offer. Subscriptions not under the Enterprise Agreement Offer are subject to denial. We will onboard eligible customers as we have space. Due to high demand, eligible customers may remain on the waitist until space is available.

          Other models ([see list](../../openai/concepts/models.md)) do not require registration.  [Learn more about limited access to Azure OpenAI Service](/legal/cognitive-services/openai/limited-access?context=/azure/cognitive-services/model-inference/context/context). 
  - name: SDKs and programming languages
    questions:
      - question: |
          Which are the supported SDKs and programming languages for Azure AI model inference service?
        answer: |
          You can use Azure Inference SDK with any model supported by the Azure AI model inference service in Azure AI services, the `AzureOpenAI` class in OpenAI SDK, or the Azure OpenAI SDK.

          Cohere SDK, Mistral SDK, and model provider-specific SDKs are not supported when connected to Azure AI services.

          See [supported SDKs and programming languages](supported-languages.md) for more information.
      - question: |
          Does Azure AI model inference service work with the latest Python library released by OpenAI (version>=1.0)?
        answer: |
          Azure AI services is supported by the latest release of the [OpenAI Python library (version>=1.0)](https://pypi.org/project/openai/). 
      - question: |
          I'm making a request for a model that is supported by Azure AI model inference service, but I'm getting a 404 error. What should I do?
        answer: |
          Ensure you have created a deployment for the given model and that the deployment name matches **exactly** the value you are passing in `model` parameter. Although routing is not case sensitive, ensure there is no special punctuation or spaces as they are common mistakes.
      - question: |
          I'm using the `azure-ai-inference` package for Python and I get a 401 error when I try to authenticate using keys. What should I do?
        answer: |
          Azure AI Services resource requires the version `azure-ai-inference>=1.0.0b5` for Python. Ensure you are using that version.
      - question: |
          I'm using OpenAI SDK and indicated the Azure OpenAI inference endpoint as base URL (https://<resource-name>.openai.azure.com). However, I get a 404 error. What should I do?
        answer: |
          Ensure you are using the correct endpoint for the Azure OpenAI service and the right set of credentials. Also, ensure that you are using the class `AzureOpenAI` from the OpenAI SDK as the authentication mechanism and URLs used are different.
      - question: |
          Does Azure AI model inference service support custom API headers? We append additional custom headers to our API requests and are seeing HTTP 431 failure errors.
        answer: |
          Our current APIs allow up to 10 custom headers, which are passed through the pipeline, and returned. We have noticed some customers now exceed this header count resulting in HTTP 431 errors. There is no solution for this error, other than to reduce header volume. In future API versions we will no longer pass through custom headers. We recommend customers not depend on custom headers in future system architectures. 
  - name: Pricing and Billing
    questions:
      - question: |
          How is Azure AI model inference service billed?
        answer: |
          You're billed for inputs and outputs to the APIs, typically in tokens. There are no cost associated with the resource itself or the deployments.

          The token price varies per each model and you are billed per 1000 tokens. You can see the pricing details before deploying a given model.
      - question: |
          Where can I see the bill details?
        answer: |
          Billing and costs are displayed in [Azure Cost Management + Billing](/azure/cost-management-billing/understand/download-azure-daily-usage). You can see the usage details in the [Azure portal](https://portal.azure.com).

          Billing isn't shown in Azure AI studio.
      - question: |
          How can I place an spending limit to my bill?
        answer: |
          You can set up a spending limit in the [Azure portal](https://portal.azure.com) under **Azure Cost Management + Billing**. This will prevent you from spending more than the limit you set. Once spending limit is reached, the subscription will be disabled and you won't be able to use the endpoint until the next billing cycle.
  - name: Data and Privacy
    questions:
      - question: |
          Do you use my company data to train any of the models? 
        answer: |
          Azure AI services doesn't use customer data to retrain models and it's never shared with model providers.    
  - name: Customer Copyright Commitment
    questions:
      - question: |
          How do I obtain coverage under the Customer Copyright Commitment? 
        answer:
          The Customer Copyright Commitment is a provision to be included in the December 1, 2023, Microsoft Product Terms that describes Microsoftâ€™s obligation to defend customers against certain third-party intellectual property claims relating to Output Content. If the subject of the claim is Output Content generated from the Azure OpenAI Service (or any other Covered Product that allows customers to configure the safety systems), then to receive coverage, customer must have implemented all mitigations required by the Azure OpenAI Service documentation in the offering that delivered the Output Content. The required mitigations are documented [here](/legal/cognitive-services/openai/customer-copyright-commitment?context=/azure/ai-services/openai/context/context) and updated on an ongoing basis. For new services, features, models, or use cases, new CCC requirements will be posted and take effect at or following the launch of such service, feature, model, or use case. Otherwise, customers will have six months from the time of publication to implement new mitigations to maintain coverage under the CCC. If a customer tenders a claim, the customer will be required to demonstrate compliance with the relevant requirements. These mitigations are required for Covered Products that allow customers to configure the safety systems, including Azure OpenAI Service; they do not impact coverage for customers using other Covered Products.
additionalContent: |